## 스파크 간단히 살펴보기
- DataFrame과 SQL을 사용해 클러스터, Spark 어플리케이션, 구조적 API를 살펴보는 챕터

### 2.1 Spark의 기본 아키텍처
- 데이터를 처리할 때 집에있는 한대의 컴퓨터를 가지고 처리하기란 어려운 일
- <b>컴퓨터 클러스터</b>는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 해줌  
  그러나 컴퓨터 클러스터를 구성하는 것만으로는 부족하며,   
  클러스터에서 작업을 조율할 수 있는 프레임워크가 필요
- Spark는 클러스터의 처리 작업을 관리, 조율함
- Spark가 연산에 사용할 클러스터는 <b>Spark stand-alone 클러스터 매니저, 하둡 YARN, 메소스 </b> 같은 클러스터 매니저에서 관리
- 사용자는 클러스터 매니저에 Spark 애플리케이션을 제출(submit)함  
  이를 받은 클러스터 매니저는 애플리케이션에 필요한 자원을 할당  
  우리는 할당받은 자원으로 작업을 처리

### 2.1.1 Spark 애플리케이션
~~~
  드라이버 프로세스         익스 큐터
--------------------      --
|  SparkSession    |     |  |
|     ^    v       |      --        --
|   사용자 코드    |               |  |
--------------------                --
              ^ v         ^ v       ^ v
-------------------------------------------------             
|            클러스터 매니저                    |
-------------------------------------------------
~~~
- 클러스터 매니저가 물리적 머신을 관리하고 스파크 애플리케이션에 자원을 할당하는 방법을 나타냄
- 클러스터 매니저는 Spark Stand-alone 클러스터 매니저, 하둡 YARN, 메소스 중 하나 선택 가능
- 하나의 클러스터에서 여러 개의 Spark 애플리케이션 실행 가능
- 사용자는 클러스터 노드에 할당할 익스큐터 수를 지정 가능 


#### Spark 애플리케이션을 이해하기 위한 핵심 사항
  - Spark는 사용 가능한 자원을 파악하기 위해 클러스터 매니저 사용
  - 드라이버 프로세스는 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터에서  
    실행할 책임이 있음

- Spark 애플리케이션은 드라이버 프로세스와 다수의 익스큐터(executor) 프로세스로 구성

#### 드라이버 프로세스
-  Spark 애플리케이션에서의 심장과 같은 존재
- 주요 역할
  - 드라이버 프로세스는 클러스터 노드 중 하나에서 실행되며 main() 함수를 실행
  - 전반적인 익스큐터 프로세스의 작업과 관련된 분석 수행
  - 스파크 애플리케이션 정보의 유지 관리
  - 사용자 프로그램이나 입력에 대한 응답
  - 배포, 스케줄링 역할 수행
- 스파크의 언어 API를 통해 다양한 언어로 실행 가능

#### 익스큐터
- 드라이버 프로세스가 할당한 작업 수행
- 드라이버가 할당한 코드를 실행하고, 진행 상황을 다시 드라이버 노드에 보고

### 2.2 스파크의 다양한 언어 API
- 스파크의 언어 API를 이용하면 다양한 프로그래밍 언어로 스파크 코드를 실행 할 수 있음
- Spark는 모든 언어에 맞는 '핵심 개념' 을 제공
- 이러한 핵심 개념은 클러스터 머신에서 실행되는 스파크 코드로 변환됨
- 다음 목록은 언어별 요약 정보임  
  - Scala
    - Spark은 Scala로 개발되어 있으므로, Scala가 Spark의 기본 언어  
  - Java
    - Spark가 Scala로 개발되어 있지만, Spark 창시자들은 Java를 이용해 Spark 코드를 작성 할 수 있도록 심혈을 기울임
  - Python
    - 스칼라가 지원하는 거의 모든 구조를 지원
  - SQL
    - Spark은 ANSI SQL:2003 표준 중 일부를 지원
    - 분석가나 비프로그래머도 SQL을 이용해 spark의 강력한 빅데이터 처리 기능 사용
  - R
    - Spark에서는 일반적으로 사용하는 2개의 라이브러리 존재
      - SparkR : Spark Core에 포함
      - sparklyr 

- SparkSession과스파크 언어 API 간의 관계
~~~
                 JVM  
           ---------------
<------   |               | <------> 파이선 프로세스
<------   | SparkSession  | <------> R 프로세스
executor로 --------------- 
전달
~~~
- 각 언어 API는 앞서 말한 핵심 개념을 유지하고 있음
- 사용자는 Spark 코드를 실행하기 위해 SparkSession을 진입점으로 사용할 수 있음
- python이나 R로 스파크를 사용할 때는 JVM 코드를 명시적으로 작성하지 않음  
- Spark은 사용자를 대신해 Python이나 R로 작성한 코드를 executor의 JVM에서 실행할 수 있는 코드로 변환

### 2.3 Spark API
- 다양한 언어로 Spark를 사용할 수 있는 이유는 Spark가 기본적으로 2가지 API를 제공하기 때문
  - 저수준의 비구조적 API
  - 고수준의 구조적 API

### 2.4 Spark 시작하기
- 실제 Spark 애플리케이션을 개발하려면 사용자 명령과 데이터를 Spark Application에 전송하는 방법을 알아야 함
- 해당 예제를 실행하려면 Spark 로컬 모드로 실행해야 함
- 대화형 모드로 Spark를 시작하면 Spark 애플리케이션을 관리하는 SparkSession이 자동으로 생성됨
- Stand-alone 애플리케이션(spark-submit)으로 Spark를 시작하면 사용자 애플리케이션 코드에서 SparkSession 객체를 직접 생성해야 함

### 2.5 SparkSession
- Spark 애플리케이션은 SparkSession이라고 불리우는 드라이버 프로세스로 제어
- SparkSession 인스턴스는 사용자가 정의한 처리 명령을 클러스터에서 실행
- 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응  
- Scala와 python 콘솔을 시작하면 spark 변수로 SparkSession을 사용 할 수 있음
~~~
spark
# 다음과 같은 결과 출력
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7ec13984
~~~
- 일정 범위의 숫자를 만드는 간단한 작업을 수행
~~~
val myRange = spark.range(1000).toDF("number")
~~~
- 위의 생성한 DataFrame은 한 개의 컬럼(column)과 1000개의 로우(row)로 구성되며  
  각 로우에는 0 ~ 999의 값이 생성
- 이 숫자들은 <b>분산 컬렉션</b>을 나타냄  
  클러스터 모드에서 코드 예제를 실행하면 숫자 범위의 각 부분이 서로 다른 익스큐터에 할당  
- 이것이 Spark의 DataFrame!

### 2.6 DataFrame
- DataFrame은 가장 대표적인 구조적 API
- DataFrame은 테이블의 데이터를 로우와 컬럼으로 단순하게 표현
- 컬럼과 컬럼의 타임을 정의한 목록을 <b>스키마(schema)</b> 라고 부름
- DataFrame을 스프레드시트와 비교할 수 있는데, 스프레드시트는 한 대의 컴퓨터에 있지만,  
  Spark DataFrame은 수천 대의 컴퓨터에 분산되어 있음  
- 여러 컴퓨터에 분산하는 이유는 데이터가 너무 크거나, 계산에 오랜 시간이 걸릴 수 있기 때문
- python과 R도 마찬가지로 DataFrame의 개념이 있지만, 단일 컴퓨터에 존재(일반적)
- 이런 상황에서는 DataFrame으로 수행할 수 있는 작업이 해당 머신이 가진 자원에 따라 제한될 수 밖에 없음
- Spark은 pandas의 DataFrame과 R의 DataFrame을 spark DataFrame으로 쉽게 변환 가능
- Spark은 Dataset, DataFrame, SQL 테이블, SDD라는 몇 가지 핵심 추상화 개념을 가지고 있음  
  이 개념 모두 분산 데이터 모음을 표현  

### 2.6.1 파티션
- Spark는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라고 불리는 청크 단위로 데이터를 분할함
- 파티션은 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미
- DataFrame의 파티션은 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타냄  
- 만약 파티션이 하나고, 수천개의 익스큐터가 있더라도 병렬성은 1
- 수백개의 파티션이 있고 익스큐터가 하나밖에 없다면 병렬성은 1
- DataFrame을 사용하면 파티션을 수동 또는 개별적으로 처리할 필요가 없음  
  물리적 파티션에 데이터 변환용 함수를 지정하면 스파크가 실제 처리 방법을 결정  



### 용어 정리
- ANSI SQL
  - DBMS(Oracle, My-SQL, DB2 등등)들에서 각기 다른 SQL를 사용하므로,  
  미국 표준 협회(American National Standards Institute)에서 이를 표준화하여  
  표준 SQL문을 정립 시켜 놓은 것  