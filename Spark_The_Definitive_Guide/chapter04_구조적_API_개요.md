## chapter04 구조적 API 개요
- 구조적 API는 비정형 로그 파일, 반정형 CSV 파일, 매우 정형적인 Parquet 파일까지  
  다양한 유형의 데이터 처리 가능
- 구조적 API에는 다음과 같은 3가지 분산 컬렉션 API가 있음
  - Dataset
  - DataFrame
  - SQL Table과 View
- <b>배치</b>와 <b>스트리밍</b>처리에서 구조적 API를 사용 가능
- 구조적 API를 활용하면 배치 <--> 스트리밍 전환 작업을 쉽게 할 수 있음
- 앞선 chapter와는 달리 구조적 API에 대해서 좀 더 자세히 알아보자
- 이번 장에서는 반드시 이해해야 하는 세 가지 기본 개념을 설명
  - 타입형(typed)/비타입형(untyped) API의 개념과 차이점
  - 핵심 용어
  - Spark가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식  

### 4.1 DataFrame과 Dataset
- Spark는 DataFrame과 Dataset이라는 두 가지 구조화된 컬렉션 개념을 가지고 있음
- DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션
- 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 함(값이 없으면 null)
- DF 와 DS는 결과를 생성하기 위해 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며 불변성을 가짐
- DF에 액션을 호출하면 스파크는 트랜스포메이션을 실제 실행하고 결과 반환
- 이 과정은 사용자가 원하는 결과를 얻기 위해 로우와 컬럼을 처리하는 방법에 관한 계획
- DF와 DS를 좀 더 구체적으로 정의하려면 <b>스키마</b>를 알아야 하는데  
  스키마는 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법

### 4.2 스키마
- 스키마는 DF의 컬럼명과 데이터타입을 정의
- 스키마는 데이터소스에서 얻거나(schema-on-read 라고 함) 직접 정의함
- 스키마는 여러 데이터 타입으로 구성되므로, 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법이 필요

### 4.3 Spark의 구조적 데이터 타입 개요
- Spark은 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 <b>카탈리스트(Catalyst)</b> 엔진을 사용
- 카탈리스트 엔진은 다양한 실행 최적화 기능 제공
- Spark는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑
- 각 언어에 대한 매핑 테이블을 가지고 있음
-  python이나 R을 이용해 Spark의 구조적 API를 사용하더라도 대부분의 연산은 Spark의 데이터 타입을 사용
- 다음 예제 코드는 Scala나 python이 아닌 Spark의 덧셈 연산 수행
~~~
// Scala
val df = spark.range(500).toDF("number")
df.select(df.col("number") + 10)
~~~
- Spark에서 덧셈연산이 수행되는 이유는 Spark가 지원하는 언어를 이용해 작성된 표현식을   
  카탈리스크 엔진에서 스파크의 데이터 타입으로 변환해 명령을 처리하기 때문

#### 4.3.1 DataFrame과 Dataset 비교
- 본질적으로 구조적 API는 '비타입형'인 DataFrame과 '타입형'인 Dataset이 있음
- 물론 DataFrame에도 데이터 타입이 있지만 스키마에 명시된 데이터 타입의 일치 여부를 <b>런타임</b> 시점에 확인
- Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 <b>컴파일 타임</b>에 확인
- Dataset은 JVM 기반 언어인 Scala와 Java에서만 지원
- Dataset의 Data-type을 지정하려면 Scala의 case class나 JavaBean을 사용해야 함
- 이 책의 예제는 대부분 DataFrame을 사용
- Spark의 DataFrame은 Row 타입으로 구성된 Dataset인데,  
  Row 타입은 Spark가 사용하는 '연산에 최적화된 인메모리 포멧'의 내부적인 표현 방식
- Row 타입을 사용하면 garbage collection과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포멧을 사용하기 때문에 매우 효율적인 연산 가능
- python이나 R에서는 Dataset을 사용할 수 없음
- 기억해야 할 것은 <b>DataFrame을 사용하면 Spark의 최적화된 내부 포멧을 사용할 수 있다는 사실</b>

#### 4.3.2 컬럼 
- 컬럼은 정수형이나 문자형 같은 <b>단순 데이터 타입</b>, 배열이나 맵 같은 <b>복합 데이터 타입</b> 그리고  <b>null 값</b>을 표현
- Spark는 데이터 타입의 모든 정보를 추적하며, 다양한 컬럼 변환 방법을 제공
- Spark의 컬럼은 테이블의 컬럼으로 생각할 수 있음

#### 4.3.3. 로우
- 데이터 레코드
- DataFrame의 레코드는 Row 타입으로 구성
- 로우는 SQL, RDD, 데이터소스에서 얻거나 직접 만들 수 있음
- 다음은 range 메소드를 이용해 DataFrame을 생성하는 예제
~~~
// Row 객체로 이루어진 배열 반환
spark.range(2).toDF().collect()
~~~

#### 4.3.4 Spark 데이터 타입
- 다양한 언어별 데이터 타입 매핑 정보를 보여줌 (p.116 ~ p.117)
- 특정 데이터 타입의 컬럼을 초기화하고 정의하는 방법을 알아보자
- Spark 데이터 타입을 스칼라에서 사용하려면 다음과 같은 코드 사용
~~~
import org.apache.spark.sql.types._
val b = ByteType
~~~
- 스칼라 데이터 타입 매핑 정보  
![img](https://github.com/koni114/Spark/blob/main/Spark_The_Definitive_Guide/imgs/scala_data_type_mapping.jpg)
- 고정형 DataFrame을 그대로 사용하는 경우는 없으며, 대부분 DataFrame의 처리와 변환을  
 수행하므로, 구조적 API의 실행 과정을 알아야 함

### 4.4 구조적 API의 실행 과정
- Spark 코드가 클러스터에서 실제 처리되는 과정을 설명
- 구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 과정을 알아보자
  1. DataFrame/Dataset/SQL을 이용해 코드 작성
  2. 정상적인 코드라면 Spark가 <b>논리적 실행 계획</b>으로 변환
  3. Spark는 <b>논리적 실행 계획</b>을 <b>물리적 실행 계획</b>으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
  4. Spark은 cluster에서 물리적 실행 계획(RDD 처리)을 실행  
- 먼저 실행할 코드를 작성해야 함
- 작성한 스파크 코드는 콘솔이나 spark-submit shell script로 실행  
- 카탈리스트 옵티마이저는 코드를 넘겨받고 실제 실행 계획을 생성
- 마지막으로 Spark는 코드를 실행 한 후 결과 반환  
~~~
----------------------------------------------------------------
|   SQL     | ---->                                 물리적      |
-------------            카탈리스트                실행 계획    |
| DataFrame | ---->                  ----->                     |
-------------            옵티마이저                             |
|  Dataset  | ---->                                             | 
-----------------------------------------------------------------
~~~

#### 4.4.1 논리적 실행 계획
- 사용자 코드를 논리적 실행 계획으로 변환
- 논리적 실행 단계에서는 추상적 트렌스포메이션만 표현
- 이 단계에서는 드라이버나 익스큐터의 정보를 고려하지 않음
- 사용자의 다양한 표현식을 최적화된 버전으로 변환
~~~
                                        (분석)                  (논리적최적화)                      
사용자 코드 ---> 검증전 논리적 실행 계획 ---> 검증된 논리적 실행 계획 --> 최적화된 
             ^                                                           논리적 실행계획
             |
           카탈로그
~~~
- 먼저 사용자 코드는 <b>검증 전 논리적 실행 계획</b>으로 변환
- 코드의 유효성과 테이블이나 컬럼의 존재 여부만을 판단하는 과정이므로 아직 실행계획을 검증하지 않은 상태
- Spark Analyzer는 컬럼과 테이블을 검증하기 위해 카탈로그, 모든 테이블의 저장소 그리고 DataFrame 정보를 활용
- 필요한 테이블이나 컬럼이 카탈로그에 없다면 검증전 논리적 실행 계획은 생성되지 않음
- 테이블과 컬럼에 대한 검증 결과는 카탈리스트 옵티마이저로 전달
- 카탈리스트 옵티마이저는 조건절 푸쉬 다운이나 선택절 구문을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음
- 필요한 경우 도메인에 최적화된 규칙을 적용할 수 있는 카탈리스트 옵티마이저의 확장형 패키지를 만들 수도 있음  

#### 4.4.2 물리적 실행 계획
- 이어 물리적 실행 계획이 진행되는데, 이 과정은 논리적 실행 계획을 클러스터 환경에서  
  실행하는 방법을 정의
- 다양한 물리적 실행 전략을 생성하고, 비용 모델을 이용해서 비교한 후 최적의 전략을 선택
- 비용을 비교하는 한가지 예는 사용하려는 테이블의 크기나 파티션 수 등의 물리적인 속성을 고려하여 지정된 조인 연산 수행에 필요한 비용을 계산하고 비교하는 것
- 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환됨
- Spark는 DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트렌스포메이션으로 컴파일함
- 따라서 Spark 를 <b>'컴파일러'</b>라고 부르기도 함

#### 4.4.3 실행
- Spark는 물리적 실행 계획을 선정한 다음 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드 실행
- Spark는 런타임에 전체 테스크나 스테이지를 제거할 수 있는 자바 바이트 코드를 생성해 추가적인 최적화 수행
- 마지막으로 Spark는 처리 결과를 사용자에게 반환

### 용어 정리
- Parquet
  - 데이터를 저장하는 방식(파일포멧) 중 하나
  - 컬럼 기반으로 데이터를 저장하여 압축율, I/O 사용률 저하, 컬럼별 적합한 인코딩이 가능
  - 트위터(Twitter)에서 개발한 포멧이며 소스코드를 공개한 이후 아파치에서 관리되고 있음